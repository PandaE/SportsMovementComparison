"""
Configuration Validation and Testing
é…ç½®éªŒè¯å’Œæµ‹è¯•å·¥å…·

Tools for validating evaluation configurations and testing the rule engine.
"""

from typing import Dict, List, Any
import json
from .evaluation_config import get_config_manager, ActionSpec
from .rule_evaluator import create_evaluator


class ConfigValidator:
    """é…ç½®éªŒè¯å™¨"""
    
    def __init__(self):
        self.config_manager = get_config_manager()
    
    def validate_all_configs(self) -> Dict[str, List[str]]:
        """éªŒè¯æ‰€æœ‰é…ç½®"""
        results = {}
        
        for key, config in self.config_manager.actions.items():
            errors = self.validate_config(config)
            if errors:
                results[key] = errors
        
        return results
    
    def validate_config(self, config: ActionSpec) -> List[str]:
        """éªŒè¯å•ä¸ªé…ç½®"""
        errors = []
        
        # 1. æ£€æŸ¥åŸºæœ¬å­—æ®µ
        if not config.name or not config.display_name:
            errors.append("ç¼ºå°‘å¿…è¦çš„åç§°å­—æ®µ")
        
        if not config.stages:
            errors.append("æ²¡æœ‰å®šä¹‰ä»»ä½•é˜¶æ®µ")
            return errors
        
        # 2. æ£€æŸ¥é˜¶æ®µæƒé‡
        total_weight = sum(stage.weight for stage in config.stages)
        if abs(total_weight - 1.0) > 0.01:
            errors.append(f"é˜¶æ®µæƒé‡æ€»å’Œ {total_weight:.3f} ä¸ç­‰äº 1.0")
        
        # 3. æ£€æŸ¥æ¯ä¸ªé˜¶æ®µ
        for i, stage in enumerate(config.stages):
            stage_errors = self._validate_stage(stage, i)
            errors.extend(stage_errors)
        
        # 4. æ£€æŸ¥æµ‹é‡é¡¹åç§°é‡å¤
        all_measurement_names = []
        for stage in config.stages:
            for measurement in stage.measurements:
                if measurement.name in all_measurement_names:
                    errors.append(f"æµ‹é‡é¡¹åç§°é‡å¤: {measurement.name}")
                all_measurement_names.append(measurement.name)
        
        return errors
    
    def _validate_stage(self, stage, stage_index: int) -> List[str]:
        """éªŒè¯é˜¶æ®µé…ç½®"""
        errors = []
        prefix = f"é˜¶æ®µ {stage_index + 1} ({stage.name})"
        
        if not stage.measurements:
            errors.append(f"{prefix}: æ²¡æœ‰å®šä¹‰æµ‹é‡é¡¹")
            return errors
        
        # æ£€æŸ¥æµ‹é‡æƒé‡
        if len(stage.measurements) > 1:
            total_weight = sum(m.weight for m in stage.measurements)
            if abs(total_weight - 1.0) > 0.01:
                errors.append(f"{prefix}: æµ‹é‡æƒé‡æ€»å’Œ {total_weight:.3f} ä¸ç­‰äº 1.0")
        
        # æ£€æŸ¥æ¯ä¸ªæµ‹é‡é¡¹
        for j, measurement in enumerate(stage.measurements):
            measurement_errors = self._validate_measurement(measurement, j, prefix)
            errors.extend(measurement_errors)
        
        return errors
    
    def _validate_measurement(self, measurement, measurement_index: int, stage_prefix: str) -> List[str]:
        """éªŒè¯æµ‹é‡é¡¹é…ç½®"""
        errors = []
        prefix = f"{stage_prefix} æµ‹é‡é¡¹ {measurement_index + 1} ({measurement.name})"
        
        # æ£€æŸ¥èŒƒå›´é€»è¾‘
        ranges = [
            ("excellent_range", measurement.excellent_range),
            ("good_range", measurement.good_range), 
            ("acceptable_range", measurement.acceptable_range)
        ]
        
        for range_name, range_tuple in ranges:
            if len(range_tuple) != 2:
                errors.append(f"{prefix}: {range_name} æ ¼å¼é”™è¯¯")
                continue
            
            min_val, max_val = range_tuple
            if min_val >= max_val:
                errors.append(f"{prefix}: {range_name} æœ€å°å€¼ä¸èƒ½å¤§äºç­‰äºæœ€å¤§å€¼")
        
        # æ£€æŸ¥ç†æƒ³å€¼æ˜¯å¦åœ¨ä¼˜ç§€èŒƒå›´å†…
        if not (measurement.excellent_range[0] <= measurement.ideal_value <= measurement.excellent_range[1]):
            errors.append(f"{prefix}: ç†æƒ³å€¼ {measurement.ideal_value} ä¸åœ¨ä¼˜ç§€èŒƒå›´å†…")
        
        # æ£€æŸ¥èŒƒå›´åŒ…å«å…³ç³»
        if not self._range_contains(measurement.good_range, measurement.excellent_range):
            errors.append(f"{prefix}: è‰¯å¥½èŒƒå›´åº”è¯¥åŒ…å«ä¼˜ç§€èŒƒå›´")
        
        if not self._range_contains(measurement.acceptable_range, measurement.good_range):
            errors.append(f"{prefix}: å¯æ¥å—èŒƒå›´åº”è¯¥åŒ…å«è‰¯å¥½èŒƒå›´")
        
        return errors
    
    def _range_contains(self, outer_range: tuple, inner_range: tuple) -> bool:
        """æ£€æŸ¥å¤–å±‚èŒƒå›´æ˜¯å¦åŒ…å«å†…å±‚èŒƒå›´"""
        return (outer_range[0] <= inner_range[0] and 
                outer_range[1] >= inner_range[1])


class EvaluatorTester:
    """è¯„ä»·å™¨æµ‹è¯•å·¥å…·"""
    
    def __init__(self):
        self.evaluator = create_evaluator()
    
    def run_basic_tests(self) -> Dict[str, Any]:
        """è¿è¡ŒåŸºç¡€æµ‹è¯•"""
        results = {
            "tests_passed": 0,
            "tests_failed": 0,
            "test_results": []
        }
        
        # æµ‹è¯•1: å®Œç¾æ•°æ®
        perfect_result = self._test_perfect_performance()
        results["test_results"].append(perfect_result)
        if perfect_result["passed"]:
            results["tests_passed"] += 1
        else:
            results["tests_failed"] += 1
        
        # æµ‹è¯•2: å·®åŠ£æ•°æ®
        poor_result = self._test_poor_performance()
        results["test_results"].append(poor_result)
        if poor_result["passed"]:
            results["tests_passed"] += 1
        else:
            results["tests_failed"] += 1
        
        # æµ‹è¯•3: æ··åˆæ•°æ®
        mixed_result = self._test_mixed_performance()
        results["test_results"].append(mixed_result)
        if mixed_result["passed"]:
            results["tests_passed"] += 1
        else:
            results["tests_failed"] += 1
        
        return results
    
    def _test_perfect_performance(self) -> Dict[str, Any]:
        """æµ‹è¯•å®Œç¾è¡¨ç°"""
        test_data = {
            "stance_width": 50,          # ç†æƒ³å€¼
            "racket_ready_angle": 110,   # ç†æƒ³å€¼
            "shoulder_rotation": 45,     # ç†æƒ³å€¼
            "elbow_height": 0,           # ç†æƒ³å€¼
            "wrist_extension": 25,       # ç†æƒ³å€¼
            "arm_extension": 160,        # ç†æƒ³å€¼
            "contact_height": 30,        # ç†æƒ³å€¼
            "body_lean": 15              # ç†æƒ³å€¼
        }
        
        try:
            result = self.evaluator.evaluate("badminton", "clear", test_data)
            
            # éªŒè¯ç»“æœ
            passed = (
                result.total_score >= 95 and  # åº”è¯¥å¾—åˆ°é«˜åˆ†
                result.level in ["excellent", "perfect"] and
                len(result.stages) == 3 and
                all(stage.stage_score >= 90 for stage in result.stages)
            )
            
            return {
                "test_name": "å®Œç¾è¡¨ç°æµ‹è¯•",
                "passed": passed,
                "score": result.total_score,
                "level": result.level,
                "details": f"å¾—åˆ†: {result.total_score:.1f}, ç­‰çº§: {result.level}"
            }
            
        except Exception as e:
            return {
                "test_name": "å®Œç¾è¡¨ç°æµ‹è¯•",
                "passed": False,
                "error": str(e),
                "details": "æµ‹è¯•æ‰§è¡Œå¤±è´¥"
            }
    
    def _test_poor_performance(self) -> Dict[str, Any]:
        """æµ‹è¯•å·®åŠ£è¡¨ç°"""
        test_data = {
            "stance_width": 80,          # è¿œè¶…å¯æ¥å—èŒƒå›´
            "racket_ready_angle": 60,    # è¿œä½äºå¯æ¥å—èŒƒå›´
            "shoulder_rotation": 100,    # è¿œè¶…å¯æ¥å—èŒƒå›´
            "elbow_height": -30,         # è¿œä½äºå¯æ¥å—èŒƒå›´
            "wrist_extension": 60,       # è¿œè¶…å¯æ¥å—èŒƒå›´
            "arm_extension": 100,        # è¿œä½äºå¯æ¥å—èŒƒå›´
            "contact_height": 60,        # è¿œè¶…å¯æ¥å—èŒƒå›´
            "body_lean": 50              # è¿œè¶…å¯æ¥å—èŒƒå›´
        }
        
        try:
            result = self.evaluator.evaluate("badminton", "clear", test_data)
            
            # éªŒè¯ç»“æœ
            passed = (
                result.total_score <= 60 and  # åº”è¯¥å¾—åˆ°ä½åˆ†
                result.level in ["poor", "needs_improvement"] and
                len(result.improvement_suggestions) > 0
            )
            
            return {
                "test_name": "å·®åŠ£è¡¨ç°æµ‹è¯•",
                "passed": passed,
                "score": result.total_score,
                "level": result.level,
                "details": f"å¾—åˆ†: {result.total_score:.1f}, ç­‰çº§: {result.level}, å»ºè®®æ•°: {len(result.improvement_suggestions)}"
            }
            
        except Exception as e:
            return {
                "test_name": "å·®åŠ£è¡¨ç°æµ‹è¯•",
                "passed": False,
                "error": str(e),
                "details": "æµ‹è¯•æ‰§è¡Œå¤±è´¥"
            }
    
    def _test_mixed_performance(self) -> Dict[str, Any]:
        """æµ‹è¯•æ··åˆè¡¨ç°"""
        test_data = {
            "stance_width": 52,          # è‰¯å¥½èŒƒå›´
            "racket_ready_angle": 108,   # ä¼˜ç§€èŒƒå›´
            "shoulder_rotation": 35,     # è‰¯å¥½èŒƒå›´
            "elbow_height": 8,           # è‰¯å¥½èŒƒå›´
            "arm_extension": 150,        # è‰¯å¥½èŒƒå›´
            "contact_height": 25,        # ä¼˜ç§€èŒƒå›´
        }
        
        try:
            result = self.evaluator.evaluate("badminton", "clear", test_data)
            
            # éªŒè¯ç»“æœ
            passed = (
                50 <= result.total_score <= 90 and  # åº”è¯¥å¾—åˆ°ä¸­ç­‰åˆ†æ•°
                result.level in ["good", "excellent"] and
                len(result.stages) == 3
            )
            
            return {
                "test_name": "æ··åˆè¡¨ç°æµ‹è¯•",
                "passed": passed,
                "score": result.total_score,
                "level": result.level,
                "details": f"å¾—åˆ†: {result.total_score:.1f}, ç­‰çº§: {result.level}"
            }
            
        except Exception as e:
            return {
                "test_name": "æ··åˆè¡¨ç°æµ‹è¯•",
                "passed": False,
                "error": str(e),
                "details": "æµ‹è¯•æ‰§è¡Œå¤±è´¥"
            }
    
    def test_single_measurement(self, measurement_name: str, test_values: List[float]) -> Dict[str, Any]:
        """æµ‹è¯•å•ä¸ªæµ‹é‡é¡¹çš„è¯„åˆ†æ›²çº¿"""
        results = []
        
        # åˆ›å»ºåŸºå‡†æ•°æ®ï¼ˆæ‰€æœ‰é¡¹ç›®éƒ½æ˜¯ç†æƒ³å€¼ï¼‰
        base_data = {
            "stance_width": 50,
            "racket_ready_angle": 110,
            "shoulder_rotation": 45,
            "elbow_height": 0,
            "wrist_extension": 25,
            "arm_extension": 160,
            "contact_height": 30,
            "body_lean": 15
        }
        
        for value in test_values:
            test_data = base_data.copy()
            test_data[measurement_name] = value
            
            try:
                result = self.evaluator.evaluate("badminton", "clear", test_data)
                
                # æ‰¾åˆ°å¯¹åº”çš„æµ‹é‡ç»“æœ
                measurement_result = None
                for stage in result.stages:
                    for measurement in stage.measurements:
                        if measurement.name == measurement_name:
                            measurement_result = measurement
                            break
                
                if measurement_result:
                    results.append({
                        "value": value,
                        "score": measurement_result.score,
                        "level": measurement_result.level,
                        "feedback": measurement_result.feedback
                    })
                else:
                    results.append({
                        "value": value,
                        "error": "æµ‹é‡é¡¹æœªæ‰¾åˆ°"
                    })
                    
            except Exception as e:
                results.append({
                    "value": value,
                    "error": str(e)
                })
        
        return {
            "measurement_name": measurement_name,
            "test_results": results
        }


def run_full_validation() -> Dict[str, Any]:
    """è¿è¡Œå®Œæ•´çš„éªŒè¯æµ‹è¯•"""
    print("ğŸ” å¼€å§‹è¯„ä»·ç³»ç»ŸéªŒè¯...")
    
    # 1. é…ç½®éªŒè¯
    print("\\n1. éªŒè¯é…ç½®æ–‡ä»¶...")
    validator = ConfigValidator()
    config_errors = validator.validate_all_configs()
    
    if config_errors:
        print("âŒ å‘ç°é…ç½®é”™è¯¯:")
        for config_name, errors in config_errors.items():
            print(f"  {config_name}:")
            for error in errors:
                print(f"    - {error}")
    else:
        print("âœ… é…ç½®éªŒè¯é€šè¿‡")
    
    # 2. è¯„ä»·å™¨æµ‹è¯•
    print("\\n2. æµ‹è¯•è¯„ä»·å¼•æ“...")
    tester = EvaluatorTester()
    test_results = tester.run_basic_tests()
    
    print(f"âœ… é€šè¿‡æµ‹è¯•: {test_results['tests_passed']}")
    print(f"âŒ å¤±è´¥æµ‹è¯•: {test_results['tests_failed']}")
    
    for test in test_results['test_results']:
        status = "âœ…" if test['passed'] else "âŒ"
        print(f"  {status} {test['test_name']}: {test['details']}")
    
    # 3. æµ‹è¯•è¯„åˆ†æ›²çº¿
    print("\\n3. æµ‹è¯•è¯„åˆ†æ›²çº¿...")
    curve_test = tester.test_single_measurement("arm_extension", [120, 140, 150, 160, 170, 180, 200])
    
    print("æ‰‹è‡‚ä¼¸å±•è§’åº¦è¯„åˆ†æ›²çº¿:")
    for result in curve_test['test_results']:
        if 'error' not in result:
            print(f"  {result['value']}Â° â†’ {result['score']:.1f}åˆ† ({result['level']})")
    
    return {
        "config_errors": config_errors,
        "test_results": test_results,
        "validation_passed": len(config_errors) == 0 and test_results['tests_failed'] == 0
    }


if __name__ == "__main__":
    # è¿è¡ŒéªŒè¯
    results = run_full_validation()
    
    if results["validation_passed"]:
        print("\\nğŸ‰ è¯„ä»·ç³»ç»ŸéªŒè¯å®Œå…¨é€šè¿‡ï¼")
    else:
        print("\\nâš ï¸ è¯„ä»·ç³»ç»ŸéªŒè¯å‘ç°é—®é¢˜ï¼Œè¯·æ£€æŸ¥ä¸Šè¿°è¾“å‡ºã€‚")