"""
Key frame extraction for sports movement analysis.
Automatically extracts important frames based on sport and action type.
"""
import cv2
import numpy as np
from typing import Dict, List, Tuple, Optional
from ...video_frame_extractor import VideoFrameExtractor
from .pose_extractor import PoseExtractor


class MotionAnalyzer:
    """è¿åŠ¨åˆ†æå™¨ - åˆ†æäººä½“è¿åŠ¨çš„è¿åŠ¨å­¦ç‰¹å¾"""
    
    def __init__(self, pose_extractor: PoseExtractor):
        self.pose_extractor = pose_extractor
    
    def analyze_video_motion(self, video_path: str, sample_rate: int = 3) -> Dict:
        """
        åˆ†æè§†é¢‘ä¸­çš„è¿åŠ¨ç‰¹å¾
        
        Args:
            video_path: è§†é¢‘è·¯å¾„
            sample_rate: é‡‡æ ·ç‡ï¼Œæ¯å‡ å¸§é‡‡æ ·ä¸€æ¬¡
            
        Returns:
            åŒ…å«è¿åŠ¨åˆ†ææ•°æ®çš„å­—å…¸
        """
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            raise ValueError(f"æ— æ³•æ‰“å¼€è§†é¢‘: {video_path}")
        
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        fps = cap.get(cv2.CAP_PROP_FPS)
        
        # å­˜å‚¨åˆ†ææ•°æ®
        motion_data = {
            'frame_numbers': [],
            'center_of_mass_x': [],  # é‡å¿ƒXåæ ‡
            'center_of_mass_y': [],  # é‡å¿ƒYåæ ‡
            'right_arm_angle': [],   # å³è‡‚è§’åº¦ï¼ˆè‚©-è‚˜-è…•ï¼‰
            'right_forearm_angle': [], # å³å‰è‡‚ç›¸å¯¹è§’åº¦
            'pose_confidence': []    # å§¿æ€æ£€æµ‹ç½®ä¿¡åº¦
        }
        
        try:
            frame_idx = 0
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                
                # æŒ‰é‡‡æ ·ç‡å¤„ç†
                if frame_idx % sample_rate == 0:
                    # æå–å§¿æ€
                    pose = self.pose_extractor.extract_pose_from_image(frame, frame_idx)
                    
                    if pose:
                        # è®¡ç®—é‡å¿ƒä½ç½®
                        com_x, com_y = self._calculate_center_of_mass(pose)
                        
                        # è®¡ç®—æ‰‹è‡‚è§’åº¦
                        arm_angle = self._calculate_arm_angle(pose)
                        forearm_angle = self._calculate_forearm_relative_angle(pose)
                        
                        # è®¡ç®—å¹³å‡ç½®ä¿¡åº¦
                        confidence = self._calculate_average_confidence(pose)
                        
                        # å­˜å‚¨æ•°æ®
                        motion_data['frame_numbers'].append(frame_idx)
                        motion_data['center_of_mass_x'].append(com_x)
                        motion_data['center_of_mass_y'].append(com_y)
                        motion_data['right_arm_angle'].append(arm_angle)
                        motion_data['right_forearm_angle'].append(forearm_angle)
                        motion_data['pose_confidence'].append(confidence)
                
                frame_idx += 1
                
                # æ˜¾ç¤ºè¿›åº¦
                if frame_idx % 30 == 0:
                    progress = frame_idx / total_frames * 100
                    print(f"   åˆ†æè¿›åº¦: {progress:.1f}% ({frame_idx}/{total_frames})")
        
        finally:
            cap.release()
        
        # è®¡ç®—è¿åŠ¨é€Ÿåº¦ï¼ˆè§’é€Ÿåº¦ç­‰ï¼‰
        motion_data = self._calculate_motion_velocities(motion_data, fps, sample_rate)
        
        return motion_data
    
    def _calculate_center_of_mass(self, pose) -> Tuple[float, float]:
        """è®¡ç®—é‡å¿ƒä½ç½®ï¼ˆåŸºäºä¸»è¦èº«ä½“å…³é”®ç‚¹ï¼‰"""
        key_points = []
        weights = []
        
        # ä¸»è¦èº«ä½“éƒ¨ä½åŠå…¶æƒé‡
        body_parts = [
            (pose.nose, 0.1),
            (pose.left_shoulder, 0.15), (pose.right_shoulder, 0.15),
            (pose.left_hip, 0.2), (pose.right_hip, 0.2),
            (pose.left_knee, 0.1), (pose.right_knee, 0.1)
        ]
        
        for point, weight in body_parts:
            if point and hasattr(point, 'x') and hasattr(point, 'y'):
                key_points.append((point.x, point.y))
                weights.append(weight)
        
        if not key_points:
            return 0.0, 0.0
        
        # åŠ æƒå¹³å‡
        total_weight = sum(weights)
        weighted_x = sum(p[0] * w for p, w in zip(key_points, weights)) / total_weight
        weighted_y = sum(p[1] * w for p, w in zip(key_points, weights)) / total_weight
        
        return weighted_x, weighted_y
    
    def _calculate_arm_angle(self, pose) -> float:
        """è®¡ç®—å³è‡‚è§’åº¦ï¼ˆè‚©-è‚˜-è…•ï¼‰"""
        if not (pose.right_shoulder and pose.right_elbow and pose.right_wrist):
            return 0.0
        
        # è®¡ç®—å‘é‡
        v1 = np.array([pose.right_shoulder.x - pose.right_elbow.x, 
                      pose.right_shoulder.y - pose.right_elbow.y])
        v2 = np.array([pose.right_wrist.x - pose.right_elbow.x, 
                      pose.right_wrist.y - pose.right_elbow.y])
        
        # è®¡ç®—è§’åº¦
        cos_angle = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))
        cos_angle = np.clip(cos_angle, -1.0, 1.0)
        angle = np.arccos(cos_angle) * 180 / np.pi
        
        return angle
    
    def _calculate_forearm_relative_angle(self, pose) -> float:
        """è®¡ç®—å‰è‡‚ç›¸å¯¹äºå‚ç›´æ–¹å‘çš„è§’åº¦"""
        if not (pose.right_elbow and pose.right_wrist):
            return 0.0
        
        # å‰è‡‚å‘é‡
        forearm_vector = np.array([pose.right_wrist.x - pose.right_elbow.x,
                                 pose.right_wrist.y - pose.right_elbow.y])
        
        # å‚ç›´å‘é‡ï¼ˆå‘ä¸‹ï¼‰
        vertical_vector = np.array([0, 1])
        
        # è®¡ç®—è§’åº¦
        cos_angle = np.dot(forearm_vector, vertical_vector) / np.linalg.norm(forearm_vector)
        cos_angle = np.clip(cos_angle, -1.0, 1.0)
        angle = np.arccos(cos_angle) * 180 / np.pi
        
        return angle
    
    def _calculate_average_confidence(self, pose) -> float:
        """è®¡ç®—å§¿æ€æ£€æµ‹çš„å¹³å‡ç½®ä¿¡åº¦"""
        key_points = [
            pose.right_shoulder, pose.right_elbow, pose.right_wrist,
            pose.left_shoulder, pose.left_hip, pose.right_hip
        ]
        
        confidences = []
        for point in key_points:
            if point and hasattr(point, 'confidence'):
                confidences.append(point.confidence)
        
        return np.mean(confidences) if confidences else 0.0
    
    def _calculate_motion_velocities(self, motion_data: Dict, fps: float, sample_rate: int) -> Dict:
        """è®¡ç®—è¿åŠ¨é€Ÿåº¦å’Œè§’é€Ÿåº¦"""
        frame_numbers = motion_data['frame_numbers']
        
        if len(frame_numbers) < 2:
            motion_data['com_velocity_x'] = [0.0]
            motion_data['com_velocity_y'] = [0.0]
            motion_data['arm_angular_velocity'] = [0.0]
            motion_data['forearm_angular_velocity'] = [0.0]
            return motion_data
        
        # è®¡ç®—æ—¶é—´é—´éš”
        dt = sample_rate / fps
        
        # è®¡ç®—é‡å¿ƒé€Ÿåº¦
        com_vel_x = np.gradient(motion_data['center_of_mass_x']) / dt
        com_vel_y = np.gradient(motion_data['center_of_mass_y']) / dt
        
        # è®¡ç®—è§’é€Ÿåº¦
        arm_angular_vel = np.gradient(motion_data['right_arm_angle']) / dt
        forearm_angular_vel = np.gradient(motion_data['right_forearm_angle']) / dt
        
        motion_data['com_velocity_x'] = com_vel_x.tolist()
        motion_data['com_velocity_y'] = com_vel_y.tolist()
        motion_data['arm_angular_velocity'] = arm_angular_vel.tolist()
        motion_data['forearm_angular_velocity'] = forearm_angular_vel.tolist()
        
        return motion_data


class KeyFrameExtractor:
    """å…³é”®å¸§æå–å™¨ - æ ¹æ®è¿åŠ¨ç±»å‹è‡ªåŠ¨æå–å…³é”®å¸§"""
    
    def __init__(self, use_intelligent_extraction: bool = True):
        """
        åˆå§‹åŒ–å…³é”®å¸§æå–å™¨
        
        Args:
            use_intelligent_extraction: æ˜¯å¦ä½¿ç”¨æ™ºèƒ½æå–ï¼ˆåŸºäºè¿åŠ¨å­¦ç‰¹å¾ï¼‰
        """
        self.frame_extractor = VideoFrameExtractor()
        self.use_intelligent_extraction = use_intelligent_extraction
        
        if self.use_intelligent_extraction:
            try:
                self.pose_extractor = PoseExtractor(backend="mediapipe")
                self.motion_analyzer = MotionAnalyzer(self.pose_extractor)
                print("ğŸ§  æ™ºèƒ½å…³é”®å¸§æå–å™¨åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                print(f"âš ï¸ æ™ºèƒ½æå–å™¨åˆå§‹åŒ–å¤±è´¥ï¼Œå›é€€åˆ°æ—¶é—´ç­‰åˆ†æ¨¡å¼: {e}")
                self.use_intelligent_extraction = False
    
    def extract_stage_frames(self, video_path: str, sport: str, action: str) -> Dict[str, int]:
        """
        æ ¹æ®è¿åŠ¨å’ŒåŠ¨ä½œç±»å‹æå–å…³é”®å¸§ä½ç½®
        
        Args:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
            sport: è¿åŠ¨ç±»å‹ (å¦‚ "badminton")
            action: åŠ¨ä½œç±»å‹ (å¦‚ "clear", "æ­£æ‰‹é«˜è¿œçƒ")
            
        Returns:
            Dict[stage_name, frame_number] - é˜¶æ®µåç§°åˆ°å¸§å·çš„æ˜ å°„
        """
        if sport.lower() == "badminton" and ("clear" in action.lower() or "æ­£æ‰‹é«˜è¿œ" in action):
            if self.use_intelligent_extraction:
                return self._extract_badminton_clear_frames_intelligent(video_path)
            else:
                return self._extract_badminton_clear_frames_simple(video_path)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„è¿åŠ¨åŠ¨ä½œç»„åˆ: {sport} - {action}")
    
    def _extract_badminton_clear_frames_intelligent(self, video_path: str) -> Dict[str, int]:
        """
        åŸºäºè¿åŠ¨å­¦ç‰¹å¾çš„ç¾½æ¯›çƒæ­£æ‰‹é«˜è¿œçƒå…³é”®å¸§æå–
        
        æå–ç­–ç•¥ï¼š
        - æ¶æ‹é˜¶æ®µç»“æŸ: é‡å¿ƒæœ€é åçš„å¸§
        - å¼•æ‹é˜¶æ®µç»“æŸ: ä¸¤ä¸ªé˜¶æ®µä¹‹é—´å°è‡‚è§’é€Ÿåº¦æœ€ä½ç‚¹  
        - å‘åŠ›é˜¶æ®µç»“æŸ: å¤§è‡‚æˆ–å°è‡‚è§’é€Ÿåº¦æœ€é«˜ç‚¹
        
        Args:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
            
        Returns:
            Dict[stage_name, frame_number] - é˜¶æ®µåˆ°å¸§å·çš„æ˜ å°„
        """
        print(f"ğŸ§  å¼€å§‹æ™ºèƒ½åˆ†æç¾½æ¯›çƒæ­£æ‰‹é«˜è¿œçƒå…³é”®å¸§...")
        
        try:
            # 1. åˆ†æè§†é¢‘è¿åŠ¨ç‰¹å¾
            motion_data = self.motion_analyzer.analyze_video_motion(video_path, sample_rate=2)
            
            if len(motion_data['frame_numbers']) < 10:
                print("âš ï¸ è¿åŠ¨æ•°æ®ä¸è¶³ï¼Œå›é€€åˆ°ç®€å•æ¨¡å¼")
                return self._extract_badminton_clear_frames_simple(video_path)
            
            # 2. æ‰¾åˆ°å…³é”®å¸§
            key_frames = self._find_badminton_key_frames(motion_data)
            
            print(f"âœ… æ™ºèƒ½å…³é”®å¸§æå–å®Œæˆ:")
            for stage_name, frame_number in key_frames.items():
                print(f"   {stage_name}: ç¬¬ {frame_number} å¸§")
            
            return key_frames
            
        except Exception as e:
            print(f"âŒ æ™ºèƒ½æå–å¤±è´¥: {e}")
            print("   å›é€€åˆ°ç®€å•æ—¶é—´ç­‰åˆ†æ¨¡å¼")
            return self._extract_badminton_clear_frames_simple(video_path)
    
    def _find_badminton_key_frames(self, motion_data: Dict) -> Dict[str, int]:
        """
        åŸºäºè¿åŠ¨æ•°æ®æ‰¾åˆ°ç¾½æ¯›çƒå…³é”®å¸§
        
        Args:
            motion_data: è¿åŠ¨åˆ†ææ•°æ®
            
        Returns:
            å…³é”®å¸§å­—å…¸
        """
        frame_numbers = motion_data['frame_numbers']
        com_x = motion_data['center_of_mass_x']
        arm_angular_vel = motion_data['arm_angular_velocity']
        forearm_angular_vel = motion_data['forearm_angular_velocity']
        
        # 1. æ¶æ‹é˜¶æ®µç»“æŸï¼šé‡å¿ƒæœ€é åçš„å¸§
        # æ‰¾åˆ°é‡å¿ƒXåæ ‡æœ€å°å€¼ï¼ˆæœ€é å·¦/åï¼‰
        setup_idx = np.argmin(com_x)
        setup_frame = frame_numbers[setup_idx]
        
        # 2. å‘åŠ›é˜¶æ®µç»“æŸï¼šå¤§è‡‚æˆ–å°è‡‚è§’é€Ÿåº¦æœ€é«˜ç‚¹
        # è®¡ç®—ç»¼åˆè§’é€Ÿåº¦ï¼ˆå¤§è‡‚å’Œå°è‡‚çš„ç»å¯¹å€¼ä¹‹å’Œï¼‰
        combined_angular_vel = [abs(a) + abs(f) for a, f in zip(arm_angular_vel, forearm_angular_vel)]
        power_idx = np.argmax(combined_angular_vel)
        power_frame = frame_numbers[power_idx]
        
        # 3. å¼•æ‹é˜¶æ®µç»“æŸï¼šæ¶æ‹å’Œå‘åŠ›ä¹‹é—´å°è‡‚è§’é€Ÿåº¦æœ€ä½ç‚¹
        # ç¡®å®šæœç´¢èŒƒå›´
        start_idx = min(setup_idx, power_idx)
        end_idx = max(setup_idx, power_idx)
        
        if start_idx == end_idx:
            # å¦‚æœä¸¤ç‚¹é‡åˆï¼Œæ‰©å¤§æœç´¢èŒƒå›´
            mid_idx = len(frame_numbers) // 2
            start_idx = max(0, mid_idx - 5)
            end_idx = min(len(frame_numbers) - 1, mid_idx + 5)
        
        # åœ¨èŒƒå›´å†…æ‰¾åˆ°å°è‡‚è§’é€Ÿåº¦æœ€ä½ç‚¹
        search_range = range(start_idx, end_idx + 1)
        forearm_vel_in_range = [abs(forearm_angular_vel[i]) for i in search_range]
        
        if forearm_vel_in_range:
            min_vel_idx = np.argmin(forearm_vel_in_range)
            backswing_idx = start_idx + min_vel_idx
            backswing_frame = frame_numbers[backswing_idx]
        else:
            # å¤‡é€‰æ–¹æ¡ˆï¼šå–ä¸­é—´å¸§
            backswing_frame = frame_numbers[len(frame_numbers) // 2]
        
        # éªŒè¯å¸§çš„æ—¶é—´é¡ºåºï¼Œç¡®ä¿åˆç†æ€§
        frames = [setup_frame, backswing_frame, power_frame]
        frames.sort()
        
        key_frames = {
            "æ¶æ‹é˜¶æ®µç»“æŸ": frames[0],
            "å¼•æ‹é˜¶æ®µç»“æŸ": frames[1], 
            "å‘åŠ›é˜¶æ®µç»“æŸ": frames[2]
        }
        
        # è°ƒè¯•ä¿¡æ¯
        print(f"   ğŸ¯ åˆ†æç»“æœ:")
        print(f"     é‡å¿ƒæœ€é å: ç¬¬ {setup_frame} å¸§ (é‡å¿ƒX: {com_x[setup_idx]:.1f})")
        print(f"     è§’é€Ÿåº¦æœ€é«˜: ç¬¬ {power_frame} å¸§ (è§’é€Ÿåº¦: {combined_angular_vel[power_idx]:.1f})")
        print(f"     å°è‡‚è§’é€Ÿåº¦æœ€ä½: ç¬¬ {backswing_frame} å¸§")
        
        return key_frames
    
    def extract_stage_images(self, video_path: str, sport: str, action: str) -> Dict[str, np.ndarray]:
        """
        æå–å…³é”®å¸§å›¾åƒ
        
        Args:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
            sport: è¿åŠ¨ç±»å‹
            action: åŠ¨ä½œç±»å‹
            
        Returns:
            Dict[stage_name, image] - é˜¶æ®µåç§°åˆ°å›¾åƒçš„æ˜ å°„
        """
        # 1. è·å–å…³é”®å¸§ä½ç½®
        frame_positions = self.extract_stage_frames(video_path, sport, action)
        
        # 2. æå–å¯¹åº”çš„å›¾åƒ
        stage_images = {}
        cap = cv2.VideoCapture(video_path)
        
        if not cap.isOpened():
            raise ValueError(f"æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶: {video_path}")
        
        try:
            for stage_name, frame_number in frame_positions.items():
                cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)
                ret, frame = cap.read()
                
                if ret:
                    stage_images[stage_name] = frame
                    print(f"âœ… æå– {stage_name} å…³é”®å¸§: ç¬¬ {frame_number} å¸§")
                else:
                    print(f"âŒ æ— æ³•æå– {stage_name} å…³é”®å¸§: ç¬¬ {frame_number} å¸§")
        
        finally:
            cap.release()
        
        return stage_images
    
    def _extract_badminton_clear_frames_simple(self, video_path: str) -> Dict[str, int]:
        """
        ç¾½æ¯›çƒæ­£æ‰‹é«˜è¿œçƒå…³é”®å¸§æå–ï¼ˆç®€å•æ—¶é—´ç­‰åˆ†æ–¹æ³•ï¼‰
        
        ä½¿ç”¨ç®€å•çš„æ—¶é—´ç­‰åˆ†ç­–ç•¥ï¼š
        - æ¶æ‹é˜¶æ®µç»“æŸ: 30% ä½ç½®
        - å¼•æ‹é˜¶æ®µç»“æŸ: 50% ä½ç½®  
        - å‘åŠ›é˜¶æ®µç»“æŸ: 80% ä½ç½®
        
        Args:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
            
        Returns:
            Dict[stage_name, frame_number] - é˜¶æ®µåˆ°å¸§å·çš„æ˜ å°„
        """
        # è·å–è§†é¢‘ä¿¡æ¯
        video_info = self.frame_extractor.get_video_info(video_path)
        total_frames = video_info.get('total_frames', 0)
        
        if total_frames == 0:
            raise ValueError(f"æ— æ³•è·å–è§†é¢‘å¸§æ•°: {video_path}")
        
        # å®šä¹‰å„é˜¶æ®µçš„æ—¶é—´æ¯”ä¾‹
        stage_ratios = {
            "æ¶æ‹é˜¶æ®µç»“æŸ": 0.30,  # 30% ä½ç½®
            "å¼•æ‹é˜¶æ®µç»“æŸ": 0.50,  # 50% ä½ç½®
            "å‘åŠ›é˜¶æ®µç»“æŸ": 0.80   # 80% ä½ç½®
        }
        
        # è®¡ç®—å„é˜¶æ®µçš„å¸§ä½ç½®
        stage_frames = {}
        for stage_name, ratio in stage_ratios.items():
            frame_number = int(total_frames * ratio)
            # ç¡®ä¿å¸§å·åœ¨æœ‰æ•ˆèŒƒå›´å†…
            frame_number = max(0, min(frame_number, total_frames - 1))
            stage_frames[stage_name] = frame_number
        
        print(f"ğŸ“ ç¾½æ¯›çƒæ­£æ‰‹é«˜è¿œçƒå…³é”®å¸§æå–å®Œæˆ (ç®€å•æ¨¡å¼):")
        print(f"   æ€»å¸§æ•°: {total_frames}")
        for stage_name, frame_number in stage_frames.items():
            ratio = frame_number / total_frames * 100
            print(f"   {stage_name}: ç¬¬ {frame_number} å¸§ ({ratio:.1f}%)")
        
        return stage_frames
    
    def validate_extracted_frames(self, video_path: str, stage_frames: Dict[str, int]) -> Dict[str, bool]:
        """
        éªŒè¯æå–çš„å…³é”®å¸§è´¨é‡
        
        Args:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
            stage_frames: é˜¶æ®µå¸§ä½ç½®æ˜ å°„
            
        Returns:
            Dict[stage_name, is_valid] - å„é˜¶æ®µå¸§çš„æœ‰æ•ˆæ€§
        """
        validation_results = {}
        cap = cv2.VideoCapture(video_path)
        
        if not cap.isOpened():
            return {stage: False for stage in stage_frames.keys()}
        
        try:
            for stage_name, frame_number in stage_frames.items():
                cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)
                ret, frame = cap.read()
                
                if ret:
                    # ç®€å•çš„è´¨é‡æ£€éªŒ
                    is_valid = self._validate_frame_quality(frame)
                    validation_results[stage_name] = is_valid
                    
                    status = "âœ… æœ‰æ•ˆ" if is_valid else "âš ï¸ è´¨é‡é—®é¢˜"
                    print(f"   {stage_name}: {status}")
                else:
                    validation_results[stage_name] = False
                    print(f"   {stage_name}: âŒ æ— æ³•è¯»å–")
        
        finally:
            cap.release()
        
        return validation_results
    
    def _validate_frame_quality(self, frame: np.ndarray) -> bool:
        """
        ç®€å•çš„å¸§è´¨é‡éªŒè¯
        
        Args:
            frame: è¾“å…¥å¸§
            
        Returns:
            bool - æ˜¯å¦ä¸ºæœ‰æ•ˆå¸§
        """
        if frame is None or frame.size == 0:
            return False
        
        # æ£€æŸ¥å›¾åƒæ˜¯å¦å¤ªæš—æˆ–å¤ªäº®
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        mean_brightness = np.mean(gray)
        
        # äº®åº¦èŒƒå›´æ£€æŸ¥ (0-255)
        if mean_brightness < 20 or mean_brightness > 240:
            return False
        
        # æ£€æŸ¥å›¾åƒæ˜¯å¦å¤ªæ¨¡ç³Š (ä½¿ç”¨æ‹‰æ™®æ‹‰æ–¯ç®—å­)
        laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()
        
        # æ¸…æ™°åº¦é˜ˆå€¼
        if laplacian_var < 50:  # ç»éªŒå€¼ï¼Œå¯è°ƒæ•´
            return False
        
        return True
    
    def get_supported_sports(self) -> List[Tuple[str, str]]:
        """
        è·å–æ”¯æŒçš„è¿åŠ¨é¡¹ç›®åˆ—è¡¨
        
        Returns:
            List[Tuple[sport, action]] - æ”¯æŒçš„è¿åŠ¨åŠ¨ä½œç»„åˆ
        """
        return [
            ("Badminton", "æ­£æ‰‹é«˜è¿œçƒ"),
            ("badminton", "clear"),
            ("badminton", "forehand_clear")
        ]
    
    def get_default_stage_ratios(self, sport: str, action: str) -> Dict[str, float]:
        """
        è·å–é»˜è®¤çš„é˜¶æ®µæ—¶é—´æ¯”ä¾‹ï¼ˆä»…ç”¨äºç®€å•æ¨¡å¼ï¼‰
        
        Args:
            sport: è¿åŠ¨ç±»å‹
            action: åŠ¨ä½œç±»å‹
            
        Returns:
            Dict[stage_name, ratio] - é˜¶æ®µåç§°åˆ°æ—¶é—´æ¯”ä¾‹çš„æ˜ å°„
        """
        if sport.lower() == "badminton" and ("clear" in action.lower() or "æ­£æ‰‹é«˜è¿œ" in action):
            return {
                "æ¶æ‹é˜¶æ®µç»“æŸ": 0.30,
                "å¼•æ‹é˜¶æ®µç»“æŸ": 0.50,
                "å‘åŠ›é˜¶æ®µç»“æŸ": 0.80
            }
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„è¿åŠ¨åŠ¨ä½œç»„åˆ: {sport} - {action}")
    
    def set_extraction_mode(self, use_intelligent: bool):
        """
        è®¾ç½®æå–æ¨¡å¼
        
        Args:
            use_intelligent: Trueä¸ºæ™ºèƒ½æ¨¡å¼ï¼ŒFalseä¸ºç®€å•æ¨¡å¼
        """
        if use_intelligent and not hasattr(self, 'motion_analyzer'):
            try:
                self.pose_extractor = PoseExtractor(backend="mediapipe")
                self.motion_analyzer = MotionAnalyzer(self.pose_extractor)
                self.use_intelligent_extraction = True
                print("ğŸ§  åˆ‡æ¢åˆ°æ™ºèƒ½æå–æ¨¡å¼")
            except Exception as e:
                print(f"âš ï¸ æ— æ³•åˆ‡æ¢åˆ°æ™ºèƒ½æ¨¡å¼: {e}")
                self.use_intelligent_extraction = False
        else:
            self.use_intelligent_extraction = use_intelligent
            mode_name = "æ™ºèƒ½æ¨¡å¼" if use_intelligent else "ç®€å•æ¨¡å¼"
            print(f"ğŸ“ åˆ‡æ¢åˆ°{mode_name}")
    
    def get_extraction_mode(self) -> str:
        """
        è·å–å½“å‰æå–æ¨¡å¼
        
        Returns:
            æ¨¡å¼æè¿°å­—ç¬¦ä¸²
        """
        return "æ™ºèƒ½æå–æ¨¡å¼" if self.use_intelligent_extraction else "ç®€å•æ—¶é—´ç­‰åˆ†æ¨¡å¼"